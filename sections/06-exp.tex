%!TEX root = ../main.tex
\section{Experiment} 
\label{sec:exp}

\subsection{Experimental Settings}

\textbf{\textit{\underline{Dataset.}}}We evalute our algorithm on 7 different real-world datasets from a diverse array of domains, whose size varying from the magnitude of $10^{3}$ to $10^{6}$ and the number of class differ from 2 to 100. We first used the dataset form CleanML, which have been performed synthetic mislabel injection with the strategy that flipping 5\% of the labels in each class. Then, we also use different kinds of dataset including image and table and dataset with large number of classes to further verify the effectiveness and scalability of our algorithm. The details are listed as follows:

\be
\item USCensus: This dataset contains 32,561 items about US Census records for adults. Each item has 14 attributes, such as age, education, sex, etc. The classification goal is to predict whether the adult earns more than \$50,000.
\item Marketing: A total of 8993 records are contained in this dataset, which information about household income including education, sex. etc. The goal is to make predictions about whether the annual family income is less than \$25,000. 
\item EEG: This dataset consists of 14,980 Electroencephalogram recordings with 14 Electroencephalogram attributes varying from AF3 to, AF4 The classification task is to predict whether the eye-state is closed or open. 
\item CIFAR-10: The dataset is a computer vision data set for universal object recognition, which contains 50000 32 X 32 RGB color pictures, a total of 10 categories. The task is to predict which kind does the picture belong to. Mislabeled data are artificially introduced by flipping labels of 40\% for each type of the dataset randomly. \textbf{(need explain?)}
\item CIFAR-100: This dataset is like CIFAR-10, except that it has a total of 100 classes, and each class contains 500 images. The classification task is also to predict the category of a given picture.
\item CovType: This is also a muti-classification dataset, which contains 7 different forest cover type. With a total of 581012 samples, and each sample consist of 54 attributes, such as Elevation, Wilderness Area, Horizontal Distance To Roadway etc.
\item Mobile Price Prediction: This is a small tabular dataset with only 2000 records. The task is to predict price range of the mobile on the basis of the information about the mobile, specification like Battery power, 3G enabled , wifi ,Bluetooth, Ram etc.\textbf{(need remove?)}
%\item \nan{If we can have a good summary of experiments, we can either add here or use a ``stitle'' section to highlight the empirical findings.}
\ee
\textbf{\textit{\underline{Method.}}}We compare our approach against several competing mislabel detection methods. First, we consider 8 baselines:

1. \textbf{K-Nearest Neighbor(KNN):} The method counts the number of inconsistencies between the label of a training instance and the labels of its surrounding neighbors. If there is strong evidence of distinction among the labels, the training instance is marked as mislabeled. This kind of method is called local learning method.

2. \textbf{Nearest Centroid Neighborhood (NCN):} This method is also belong to local learning method, which assumes that the labels of mislabeled instances tend to disagree with the labels of other instances in their surrounding neighborhood. The difference between KNN and this method is the approach about how to find the nearest neighbor.

3. \textbf{Training Set Debugging Using Trusted Items(DUTI):} DUTI utilizes a small set of additional "trusted items"to help detect incorrectly labeled item, which core is to finds the smallest changes for the labels in training set such that the classifier trained on the changed dtaset classify all the trusted items correctly.

4. \textbf{Forgetting Events:} In the process of model training, a sample has been correctly classified by the model. With the update of model parameters, the sample has been incorrectly classified. This process is called a forgetting event of the sample. This method identifies a mislabel sample based on one assumption that noisy sample often experience more forgetting events than normal samples during model training. 

5. \textbf{Ensemble-based method with consensus filter:}  Ensemble-based method assumes that multiple, independent clissifiers often result in conflicting labels about incorrectly labeled training sample. Algorithms that belong to this category vary in terms of how the different classifiers are constructed. Besides, the consensus filter is a strategy which means that a training example could be marked as a mislabel data only if it is misclassified by all the classifiers in the ensemble.

6. \textbf{Ensemble-based method with majority vote:}  The main idea of this method is tha same as the Ensemble-based method with consensus filter.The only inconsistency between them is that majority vote strategy considers an example to be mislabeled if it disagrees with the majority vote of the classifiers.

7. \textbf{Cleanlab:} Cleanlab proposes an emerging framework called confident learning, which core for detection include two main steps: Count and Clean. The first step is to estimate a joint distribution directly between incorrectly labeled data and uncorrupted(real) data based on the assumption of classification noise process(CNP). After that, some instances in the training set would be tabbed as mislabel based on the joint distribution matrix computed above in the second step.

8. \textbf{Noisy Cross-Validation(NCV):} This method first randomly devide a noisy training set into two halves, then train a neural network for these two half separately. After that, the network which is trained on one half will be applied to another half of the dataset. A sample would be identified as mislabel when its current label is different from its predicted label.

9. \textbf{Iterative Noisy Cross-Validation(INCV):} Obviously, this is a iterative method about noisy cross-validation. Apart from selecting mislabel samples, the INCV removes samples that have large categorical cross entropy loss at each iteration.

10. \textbf{Self-Ensemble Label Filtering(SELF):} The method 

11. \textbf{Partition Filter:} This method first partition a noisy dataset into multi-subsets, and then construct a good classifier from each subset. For a given training sample, all classifier will be applied on it, the mislabel sample often have higher probability to have larger misclassified times.

We also consider three variants of our algorithm. The goal is to compare the effectiveness of the cross-validation using both early loss and parameter influence. For all variations, we use the same training paradigms. We consider the following variants:

1. \textbf{MisDetect Without Iteration: } This variant just does the detection once, which means choicing a very big dirty pool containing instances with large early loss and a clean pool to test the influence of each instance, then directly filter all the mislabel instances with large influence in the dirty pool without iteration.

2. \textbf{MisDetect Without Parameter Influence:} Apparently, this variant does not test the parameter influence after dirty pool is established at each iteration. It just consider the loss of each instance at initial training stage.

3. \textbf{MisDetect Without Early Loss:} This variant using parameter influence as the main detect method, in other words, it does not consider the loss of every instance in the training set except needing a clean pool which is created through early loss, the instance with large influence will be detected as mislabel data in this variant.

\textbf{\textit{\underline{Hyper-prameter Setting.}}} We train convolution neural network for image-classification task and simple 3-layers perceptron for tabular dataset.Both networks apply learning rate with 0.001 and optimizer with Adam. For CIFAR-10, CIFAR-100, CovType and Mobile Price Prediction datasets, we inject mislabel data by flipping labels of 20\% for each type of the dataset randomly. For MisDetect, we chose XXX as the clean pool len.For Ensemble-based method, we use a toal of 10 different models,such as AdaBoost, RandomForest, Support Vector Machine, etc. For Cleanlab, we use LogisticRegression to compute the thresholds for noisy estimation.\textbf{(need?)}

\textbf{\textit{\underline{Evaluation Metrics.}}}We mainly focus on the effectiveness of our algorithm and other baselines, so we take precision, recall and F1-score as the most important metrics.

\subsection{Comparison with Baselines}

Given that MisDetect is a iterative detection method,

\subsection{Mislabel Ratio Evaluation}

\subsection{Mislabel Distribution Evaluation}

We test the algorithms on two types of label noise: symmetric and class-conditional label noise. In symmetric label noise, each label is randomly flipped to any of the remaining classes with equal probability, whereas for class-conditional noise, label flipping is done in a set of similar classes. For example, 

\subsection{Model Evaluation}

\subsection{Stop Condition Evaluation}
