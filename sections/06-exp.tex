%!TEX root = ../main.tex
\section{Experiment} 
\label{sec:exp}

\subsection{Experimental Settings}

\textbf{\textit{\underline{Dataset.}}}We evalute our algorithm on 7 different real-world datasets from a diverse array of domains, whose size varying from the magnitude of $10^{3}$ to $10^{6}$ and the number of class differ from 2 to 100. We first used the dataset form CleanML, which have been performed synthetic mislabel injection with the strategy that flipping 5\% of the labels in each class. Then, we also use different kinds of dataset, e.g. image,and dataset with large number of classes to further verify the effectiveness and scalability of our algorithm. The details are listed as follows:

\be
\item USCensus: This dataset contains 32,561 items about US Census records for adults. Each item has 14 attributes, such as age, education, sex, etc. The classification goal is to predict whether the adult earns more than \$50,000.
\item Marketing: The dataset is about household income,which consists of 8,993 records. A totoal of 14 demo- graphic attributes of each record varying form education to sex. The classification task is to predict if the annual household income is less than \$25,000. 
\item EEG: This is a dataset of 14,980 Electroencephalogram recordings with 14 Electroencephalogram attributes e.g., AF3, AF4. The classification task is to predict whether the eye-state is closed or open. 
\item CIFAR-10: The dataset is a computer vision data set for universal object recognition, which contains 50000 32 X 32 RGB color pictures, a total of 10 categories. The task is to predict which kind does the picture belong to. Mislabeled data are artificially introduced by flipping labels of 40\% for each type of the dataset randomly. \textbf{(need explain?)}
\item CIFAR-100: This dataset is like CIFAR-10, except that it has a total of 100 classes, and each class contains 500 images. The classification task is also to predict the category of a given picture.
\item CovType: This is also a muti-classification dataset, which contains 7 different forest cover type. With a total of 581012 samples, and each sample consist of 54 attributes, such as Elevation, Wilderness Area, Horizontal Distance To Roadway etc.
\item Mobile Price Prediction: This is a small tabular dataset with only 2000 records. The task is to predict price range of the mobile on the basis of the information about the mobile, specification like Battery power, 3G enabled , wifi ,Bluetooth, Ram etc.\textbf{(need remove?)}
\item \nan{If we can have a good summary of experiments, we can either add here or use a ``stitle'' section to highlight the empirical findings.}
\ee
\textbf{\textit{\underline{Method.}}}We compare our approach against several competing mislabel detection methods. First, we consider 8 baselines:

1. \textbf{K-Nearest Neighbor(KNN):} The method counts the number of inconsistencies between the label of a training instance and the labels of its surrounding neighbors. If there is strong evidence of distinction among the labels, the training instance is marked as mislabeled. This kind of method is called local learning method.

2. \textbf{Nearest Centroid Neighborhood (NCN):} This method is also belong to local learning method, which assumes that the labels of mislabeled instances tend to disagree with the labels of other instances in their surrounding neighborhood. The difference between KNN and this method is the approach about how to find the nearest neighbor.

3. \textbf{Training Set Debugging Using Trusted Items(DUTI):} DUTI utilizes a small set of additional "trusted items"to help detect incorrectly labeled item, which core is to finds the smallest changes for the labels in training set such that the classifier trained on the changed dtaset classify all the trusted items correctly.

4. \textbf{Ensemble-based method with consensus filter:}  Ensemble-based method assumes that multiple, independent clissifiers often result in conflicting labels about incorrectly labeled training sample. Algorithms that belong to this category vary in terms of how the different classifiers are constructed. Besides, the consensus filter is a strategy which means that a training example could be marked as a mislabel data only if it is misclassified by all the classifiers in the ensemble.

5. \textbf{Ensemble-based method with majority vote:}  The main idea of this method is tha same as the Ensemble-based method with consensus filter.The only inconsistency between them is that majority vote strategy considers an example to be mislabeled if it disagrees with the majority vote of the classifiers.

6. \textbf{Cleanlab:} 

7. \textbf{Noisy Cross-Validation(NCV):} This method first randomly devide a noisy training set into two halves, then train a neural network for these two half separately. After that, the network which is trained on one half will be applied to another half of the dataset. A sample would be identified as mislabel when its current label is different from its predicted label.

8. \textbf{Iterative Noisy Cross-Validation(INCV):} Obviously, this is a iterative method about noisy cross-validation. Apart from selecting mislabel samples, the INCV removes samples that have large categorical cross entropy loss at each iteration.

9. \textbf{Self-Ensemble Label Filtering(SELF):} The method 

10. \textbf{Partition Filter:} This method first partition a noisy dataset into multi-subsets, and then construct a good classifier from each subset. For a given training sample, all classifier will be applied on it, the mislabel sample often have higher probability to have larger misclassified times.

We also consider three variants of our algorithm. The goal is to compare the effectiveness of the cross-validation using both early loss and parameter influence. For all variations, we use the same training paradigms. We consider the following variants:

1. \textbf{Without Iteration: } 

2. \textbf{Without Parameter influence:} 

3. \textbf{Without Early Loss}

\textbf{\textit{\underline{Evaluation Metrics.}}}We mainly focus on the effectiveness of our algorithm and other baselines, so we take f1-score as the most important metric.

\subsection{Comparison with Baselines Settings}

\subsection{Mislabel Ratio Evaluation}

\subsection{Mislabel Distribution Evaluation}

\subsection{Model Evaluation}

\subsection{Stop Condition Evaluation}
