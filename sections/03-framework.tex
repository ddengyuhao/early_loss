%!TEX root = ../main.tex
\section{Framework} 
\label{sec:framework}

To address the above challenges, we design the \sys framework that introduces the early loss to well distinguish mislabeled instances from clean ones. As shown in Figure~\ref{fig:framework}, the basic idea of \sys is that with the number of epochs increases from the beginning, at each epoch, we can regard a number of  instances with small loss as clean instances, over which we  can train a model almost without  dirty data information. Then we can leverage the parameter of this model as a replacement of $w^*$. Meanwhile, we can also let a number of  instances with large  loss constitute  a pool of mislabeled instance candidates, among which we pick some  instances with high influence $\wrt$ $w^*$. Afterwards, we remove these instances iteratively, leading to more accurate mislabel detection for further epochs. 

\stitle{Early loss based detection.} As discussed in Section~\ref{sec:intro},  mislabeled instances tend to have a large early loss because the model is hard to fit them, especially at early training stages. In Figure~\ref{fig:framework}, we can see that we detect mislabels iteratively  as the training progresses.
Specifically, at the  $j$-th  epoch,  we define the loss of the $i$-th instance  as  $l_i^j = y_i - w_j\cdot x_i$, where $w_j$ is the model parameter of this epoch. Then we include a proportion of instances with  large loss as the candidate pool of mislabeled instances, denoted by $\dirtypool$, because they are likely to be dirty. Similarly, the instances with small loss are very likely to be clean because they can be well fitted by the model at early epochs.  Therefore, we construct another  pool $\cleanpool$ of clean  instances, over which we train a model to measure the influences of instances in $\dirtypool$. This will be briefly introduced next. Note that we fix  $\cleanpool$ after the first epoch because these instances that have already been fitted with small loss will not vary much afterwards, so  we do not have to iteratively update it considering the efficiency. However, $\dirtypool$ has to be dynamically updated since the loss is likely to vary when it is large at the beginning.


 %Then, for  $L_j = \{l_i^j| i\in[1,N]\}$, we can compute the standard deviation $\sigma_j = \sqrt{\frac{\sum_{i=1}^{N}(l_i^j-\mu_j)^2}{N}}$, where $\mu_j$ is the average of $L_j$. 
\stitle{Influence evaluation.} Given $\dirtypool$, a straightforward method is to   just directly remove all instances in it or someones with the largest loss, but although the removed instances tend to be mislabeled, they may not influence the current model much. Recap that in Equation~\ref{eqa:optimize},  we aim to remove mislabeled instances such that  the model can be close to $w^*$. And the more quickly (with fewer epochs) we approach  $w^*$, the easier for the model to detect mislabels because $w^*$ is the ideal parameter  trained over all clean instances. 

Since we cannot know  $w^*$ in advance, in order to measure the influence of removing each instance $w.r.t.$ approaching  $w^*$, we  convert it to  measuring the influence of removing the instance $w.r.t.$ the model trained with current dataset. Although we can compute this influence efficiently without retraining using the influence function~\cite{}, it does not perform well. The reason is that  such strategy cannot provide evident signal because current dataset is mixed with dirty and clean instances, and thus no matter how large the influence of the instance in fact is, the  result  calculated by the influence function is not obvious.

Therefore, we propose a new influence computation method customized to our problem. As introduced before, based on the early loss,  we construct a clean pool $\cleanpool$, in which the instances are very confidently clean. We train a model $M_c$ with parameter $w_c$ over $\cleanpool$. For each instance $o_i$, we measure its influence by adding $o_i$ to $\cleanpool$. Since $\cleanpool$ is clean, adding a dirty instance will be much more sensitive than the aforementioned method.
Intuitively, if an instance has a large impact on a model after adding it into the dataset  $w.r.t.$  $w_c$, removing it will be rather helpful  for   quickly  approaching $w^*$ because $w_c$ can be regarded as an approximation of $w^*$. We will introduce this in detail in Section~\ref{sec:influence}.




\stitle{False positive elimination.} During the process of mislabel detection, there inevitably exist some false positive instances, \ie  they are in fact clean but are identified as mislabeled ones. One main reason is that there exist some outliers in $\train$, which will also produce large loss at early stages of training. To address this, we propose to detect outliers before training, and temporarily put them on hold without participating in mislabel detection during training. In this way, these outliers will not be identified as false positives due to their large loss. When the mislabel detection process terminates, we can leverage the mislabeled/clean instances that have already been detected to check which ones among these outliers are mislabeled.

On the other hand, there will also be some clean instances that are not classified as outliers, but they could produce large loss as they are hard to fit for a model. We also expect to identify these false positives. In practice, we observe that these instances are easier to fit than those mislabeled ones, so we propose to address this problem by tracking the variation of loss. Intuitively, if the loss  of an instance with initial large loss quickly decreases, it tends to be a false positive. Otherwise, we still keep it as a mislabeled instance. We will discuss how to eliminate false positives in detail in Section~\ref{sec:fp}.


\stitle{Overall \sys Framework.} To summarize, given a train set $\train$, we first identify a subset of  outliers $\outlier$ in it and temporarily remove these outliers from $\train$ (\ie $\train = \train/ \outlier$).
Next, an important step is to estimate the number of mislabeled instances in $\train$ such that we can decide when to stop the detection process (Section!\ref{sec:loss}).
 Afterwards, we iterative train over $\train$ and detect mislabeled instances in each epoch, as shown in Figure~\ref{fig:framework}. In each epoch, we can obtain the loss of each instance. Then, we will extract the top largest ones to constitute $\dirtypool$, and the bottom  smallest ones to constitute $\cleanpool$. On the basis of $\cleanpool$, we can well evaluate the influence of each instance in $\dirtypool$ and select the top-$k$ instances with high influence as mislabeled ones. For example, at current epoch in Figure~\ref{fig:framework}, $\dirtypool$ consists of $o_{10}, o_{8}, o_{11}, o_{1}, o_{3}$. Next, we evaluate the influence. Although $o_{8}$ is a clean instance with a large loss, its influence is not large if we put it into $\cleanpool$ that is full of clean instances. Hence, $o_{8}$ will not be detected in this epoch, and $o_{1},o_{10}$ with high influence will be detected and removed. Then, another epoch starts, and this repeats until we have detected the number of estimated mislabels. Note that during the training process, we will also track the loss variation of each instance to identify some candidates that we suspect they are false positives.
 Finally, we will look back to these suspicious candidates as well as  $\outlier$ to check whether they are mislabeled to further improve the detection accuracy.




%\nan{I found that many reviewers will complain that (1) the above figure is too complicated to understand, and (2) the explanation of the figure is not clear. If the reviewers can fully understand and appreciate this framework figure and we have good empirical results, it is more than 50\% done.}

%\subsection{System Overview}

% \begin{itemize}
% 	\item (Motivation) Why use RL to solve the data generation problem.
% 	\item How to use RL to solve this problem.
% 	\item The workflow of the system, \ie the interaction between agent and environment.
% \end{itemize}
